{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión de Leave-One-Out Cross-Validation: 0.96\n",
      "Matriz de Confusión Leave-One-Out:\n",
      "[[50  0  0]\n",
      " [ 0 47  3]\n",
      " [ 0  3 47]]\n",
      "Precisión de Hold-Out Validation (70-30): 1.00\n",
      "Matriz de Confusión Hold-Out:\n",
      "[[19  0  0]\n",
      " [ 0 13  0]\n",
      " [ 0  0 13]]\n",
      "Precisión de 10-Fold Cross-Validation Estratificado: 0.96\n",
      "Matriz de Confusión 10-Fold Cross-Validation:\n",
      "[[20 15 15]\n",
      " [15 19 16]\n",
      " [15 16 19]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut, train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Cargar el dataset\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Polar\\\\Documents\\\\ESCUELA\\\\5TO_SEMESTRE\\\\MAKINITAS\\\\practica6\\\\iris\\\\iris.csv\")\n",
    "\n",
    "# Codificar la columna 'class' como valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "data['class'] = label_encoder.fit_transform(data['class'])\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X = data.drop(columns=['class']).values\n",
    "y = data['class'].values\n",
    "\n",
    "# Función para calcular la distancia Euclidiana\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "# Función para clasificar un nuevo punto usando el clasificador 1-NN\n",
    "def classify_1nn(new_point, X_train, y_train):\n",
    "    min_distance = float('inf')\n",
    "    closest_class = None\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        distance = euclidean_distance(new_point, X_train[i])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_class = y_train[i]\n",
    "    \n",
    "    return closest_class\n",
    "\n",
    "# --- Leave-One-Out Cross-Validation ---\n",
    "loo = LeaveOneOut()\n",
    "correct_predictions_loo = 0\n",
    "y_pred_loo = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    # Dividir datos en entrenamiento y prueba\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Clasificar el punto de prueba usando 1-NN\n",
    "    predicted_class = classify_1nn(X_test[0], X_train, y_train)\n",
    "    y_pred_loo.append(predicted_class)\n",
    "    \n",
    "    # Comparar con la clase real\n",
    "    if predicted_class == y_test[0]:\n",
    "        correct_predictions_loo += 1\n",
    "\n",
    "# Calcular y mostrar la precisión de Leave-One-Out\n",
    "accuracy_loo = correct_predictions_loo / len(X)\n",
    "print(f\"Precisión de Leave-One-Out Cross-Validation: {accuracy_loo:.2f}\")\n",
    "\n",
    "# Imprimir la matriz de confusión para Leave-One-Out\n",
    "print(\"Matriz de Confusión Leave-One-Out:\")\n",
    "print(confusion_matrix(y, y_pred_loo))\n",
    "\n",
    "# --- Hold-Out Validation (70-30 split) ---\n",
    "# Dividir en entrenamiento y prueba con un 70-30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Validar en el conjunto de prueba\n",
    "correct_predictions_holdout = 0\n",
    "y_pred_holdout = []\n",
    "\n",
    "for i, test_point in enumerate(X_test):\n",
    "    predicted_class = classify_1nn(test_point, X_train, y_train)\n",
    "    y_pred_holdout.append(predicted_class)\n",
    "    if predicted_class == y_test[i]:\n",
    "        correct_predictions_holdout += 1\n",
    "\n",
    "# Calcular y mostrar la precisión de Hold-Out\n",
    "accuracy_holdout = correct_predictions_holdout / len(X_test)\n",
    "print(f\"Precisión de Hold-Out Validation (70-30): {accuracy_holdout:.2f}\")\n",
    "\n",
    "# Imprimir la matriz de confusión para Hold-Out\n",
    "print(\"Matriz de Confusión Hold-Out:\")\n",
    "print(confusion_matrix(y_test, y_pred_holdout))\n",
    "\n",
    "# --- 10-Fold Cross-Validation Estratificado ---\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "correct_predictions_kfold = 0\n",
    "total_test_samples = 0\n",
    "y_pred_kfold = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Dividir datos en entrenamiento y prueba para el fold actual\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Validar en el conjunto de prueba\n",
    "    for i, test_point in enumerate(X_test):\n",
    "        predicted_class = classify_1nn(test_point, X_train, y_train)\n",
    "        y_pred_kfold.append(predicted_class)\n",
    "        if predicted_class == y_test[i]:\n",
    "            correct_predictions_kfold += 1\n",
    "    \n",
    "    # Actualizar el conteo de muestras de prueba\n",
    "    total_test_samples += len(X_test)\n",
    "\n",
    "# Calcular y mostrar la precisión de 10-Fold Cross-Validation\n",
    "accuracy_kfold = correct_predictions_kfold / total_test_samples\n",
    "print(f\"Precisión de 10-Fold Cross-Validation Estratificado: {accuracy_kfold:.2f}\")\n",
    "\n",
    "# Imprimir la matriz de confusión para 10-Fold Cross-Validation\n",
    "print(\"Matriz de Confusión 10-Fold Cross-Validation:\")\n",
    "print(confusion_matrix(y, y_pred_kfold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
      "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
      "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# This is a subset of the original data available at kaggle.\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Polar\\\\Documents\\\\ESCUELA\\\\5TO_SEMESTRE\\\\MAKINITAS\\\\practica6\\\\winequality\\\\winequality-red.csv\", delimiter=\";\")\n",
    "\n",
    "data.head()\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión de Leave-One-Out Cross-Validation: 0.62\n",
      "Matriz de Confusión Leave-One-Out:\n",
      "[[  1   4   3   1   1   0]\n",
      " [  4   2  19  23   3   2]\n",
      " [  3  13 475 165  24   1]\n",
      " [  1  18 163 394  52  10]\n",
      " [  0   2  23  60 110   4]\n",
      " [  0   0   2  11   3   2]]\n",
      "Precisión de Hold-Out Validation (70-30): 0.53\n",
      "Matriz de Confusión Hold-Out:\n",
      "[[  0   1   0   0   0   0]\n",
      " [  0   1   3  12   0   1]\n",
      " [  1   6 114  62  11   1]\n",
      " [  0   4  68 108  18   2]\n",
      " [  0   1   3  23  28   6]\n",
      " [  0   0   1   3   1   1]]\n",
      "Precisión de 10-Fold Cross-Validation Estratificado: 0.61\n",
      "Matriz de Confusión 10-Fold Cross-Validation:\n",
      "[[  1   0   4   4   1   0]\n",
      " [  0   1  23  18   9   2]\n",
      " [  1  13 299 279  80   9]\n",
      " [  4  18 262 263  84   7]\n",
      " [  1   4  87  78  28   1]\n",
      " [  0   0   9   8   1   0]]\n"
     ]
    }
   ],
   "source": [
    "# Codificar la columna 'class' como valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "data['quality'] = label_encoder.fit_transform(data['quality'])\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X = data.drop(columns=['quality']).values\n",
    "y = data['quality'].values\n",
    "\n",
    "# Función para calcular la distancia Euclidiana\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "# Función para clasificar un nuevo punto usando el clasificador 1-NN\n",
    "def classify_1nn(new_point, X_train, y_train):\n",
    "    min_distance = float('inf')\n",
    "    closest_class = None\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        distance = euclidean_distance(new_point, X_train[i])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_class = y_train[i]\n",
    "    \n",
    "    return closest_class\n",
    "\n",
    "# --- Leave-One-Out Cross-Validation ---\n",
    "loo = LeaveOneOut()\n",
    "correct_predictions_loo = 0\n",
    "y_pred_loo = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    # Dividir datos en entrenamiento y prueba\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Clasificar el punto de prueba usando 1-NN\n",
    "    predicted_class = classify_1nn(X_test[0], X_train, y_train)\n",
    "    y_pred_loo.append(predicted_class)\n",
    "    \n",
    "    # Comparar con la clase real\n",
    "    if predicted_class == y_test[0]:\n",
    "        correct_predictions_loo += 1\n",
    "\n",
    "# Calcular y mostrar la precisión de Leave-One-Out\n",
    "accuracy_loo = correct_predictions_loo / len(X)\n",
    "print(f\"Precisión de Leave-One-Out Cross-Validation: {accuracy_loo:.2f}\")\n",
    "\n",
    "# Imprimir la matriz de confusión para Leave-One-Out\n",
    "print(\"Matriz de Confusión Leave-One-Out:\")\n",
    "print(confusion_matrix(y, y_pred_loo))\n",
    "\n",
    "# --- Hold-Out Validation (70-30 split) ---\n",
    "# Dividir en entrenamiento y prueba con un 70-30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Validar en el conjunto de prueba\n",
    "correct_predictions_holdout = 0\n",
    "y_pred_holdout = []\n",
    "\n",
    "for i, test_point in enumerate(X_test):\n",
    "    predicted_class = classify_1nn(test_point, X_train, y_train)\n",
    "    y_pred_holdout.append(predicted_class)\n",
    "    if predicted_class == y_test[i]:\n",
    "        correct_predictions_holdout += 1\n",
    "\n",
    "# Calcular y mostrar la precisión de Hold-Out\n",
    "accuracy_holdout = correct_predictions_holdout / len(X_test)\n",
    "print(f\"Precisión de Hold-Out Validation (70-30): {accuracy_holdout:.2f}\")\n",
    "\n",
    "# Imprimir la matriz de confusión para Hold-Out\n",
    "print(\"Matriz de Confusión Hold-Out:\")\n",
    "print(confusion_matrix(y_test, y_pred_holdout))\n",
    "\n",
    "# --- 10-Fold Cross-Validation Estratificado ---\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "correct_predictions_kfold = 0\n",
    "total_test_samples = 0\n",
    "y_pred_kfold = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Dividir datos en entrenamiento y prueba para el fold actual\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Validar en el conjunto de prueba\n",
    "    for i, test_point in enumerate(X_test):\n",
    "        predicted_class = classify_1nn(test_point, X_train, y_train)\n",
    "        y_pred_kfold.append(predicted_class)\n",
    "        if predicted_class == y_test[i]:\n",
    "            correct_predictions_kfold += 1\n",
    "    \n",
    "    # Actualizar el conteo de muestras de prueba\n",
    "    total_test_samples += len(X_test)\n",
    "\n",
    "# Calcular y mostrar la precisión de 10-Fold Cross-Validation\n",
    "accuracy_kfold = correct_predictions_kfold / total_test_samples\n",
    "print(f\"Precisión de 10-Fold Cross-Validation Estratificado: {accuracy_kfold:.2f}\")\n",
    "\n",
    "# Imprimir la matriz de confusión para 10-Fold Cross-Validation\n",
    "print(\"Matriz de Confusión 10-Fold Cross-Validation:\")\n",
    "print(confusion_matrix(y, y_pred_kfold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n",
      "       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n",
      "       'pH', 'sulphates', 'alcohol', 'quality'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# This is a subset of the original data available at kaggle.\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\Polar\\\\Documents\\\\ESCUELA\\\\5TO_SEMESTRE\\\\MAKINITAS\\\\practica6\\\\winequality\\\\winequality-white.csv\", delimiter=\";\")\n",
    "\n",
    "data.head()\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión de Leave-One-Out Cross-Validation: 0.62\n",
      "Matriz de Confusión Leave-One-Out:\n",
      "[[   2    4    9    4    1    0    0]\n",
      " [   6   35   50   49   21    2    0]\n",
      " [   7   37  921  380  100   11    1]\n",
      " [   1   42  358 1474  273   49    1]\n",
      " [   2    9   76  264  504   23    2]\n",
      " [   0    1   13   52   27   82    0]\n",
      " [   0    0    2    2    1    0    0]]\n",
      "Precisión de Hold-Out Validation (70-30): 0.56\n",
      "Matriz de Confusión Hold-Out:\n",
      "[[  1   1   3   1   1   0   0]\n",
      " [  3   8  16  10   3   0   0]\n",
      " [  3  12 245 134  29   3   0]\n",
      " [  0  10 145 412  85  15   1]\n",
      " [  0   5  32  94 136  13   0]\n",
      " [  0   0   2  17  10  20   0]\n",
      " [  0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\envs\\K-means-clustering\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=10.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precisión de 10-Fold Cross-Validation Estratificado: 0.59\n",
      "Matriz de Confusión 10-Fold Cross-Validation:\n",
      "[[  0   0   7   5   7   1   0]\n",
      " [  0   7  50  70  30   5   1]\n",
      " [  6  54 419 630 301  46   1]\n",
      " [  9  56 656 995 404  77   1]\n",
      " [  5  13 259 416 157  29   1]\n",
      " [  0   3  53  72  40   7   0]\n",
      " [  0   1   0   1   2   1   0]]\n"
     ]
    }
   ],
   "source": [
    "# Codificar la columna 'class' como valores numéricos\n",
    "label_encoder = LabelEncoder()\n",
    "data['quality'] = label_encoder.fit_transform(data['quality'])\n",
    "\n",
    "# Separar características y etiquetas\n",
    "X = data.drop(columns=['quality']).values\n",
    "y = data['quality'].values\n",
    "\n",
    "# Función para calcular la distancia Euclidiana\n",
    "def euclidean_distance(x, y):\n",
    "    return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "# Función para clasificar un nuevo punto usando el clasificador 1-NN\n",
    "def classify_1nn(new_point, X_train, y_train):\n",
    "    min_distance = float('inf')\n",
    "    closest_class = None\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        distance = euclidean_distance(new_point, X_train[i])\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_class = y_train[i]\n",
    "    \n",
    "    return closest_class\n",
    "\n",
    "# --- Leave-One-Out Cross-Validation ---\n",
    "loo = LeaveOneOut()\n",
    "correct_predictions_loo = 0\n",
    "y_pred_loo = []\n",
    "\n",
    "for train_index, test_index in loo.split(X):\n",
    "    # Dividir datos en entrenamiento y prueba\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Clasificar el punto de prueba usando 1-NN\n",
    "    predicted_class = classify_1nn(X_test[0], X_train, y_train)\n",
    "    y_pred_loo.append(predicted_class)\n",
    "    \n",
    "    # Comparar con la clase real\n",
    "    if predicted_class == y_test[0]:\n",
    "        correct_predictions_loo += 1\n",
    "\n",
    "# Calcular y mostrar la precisión de Leave-One-Out\n",
    "accuracy_loo = correct_predictions_loo / len(X)\n",
    "print(f\"Precisión de Leave-One-Out Cross-Validation: {accuracy_loo:.2f}\")\n",
    "\n",
    "# Imprimir la matriz de confusión para Leave-One-Out\n",
    "print(\"Matriz de Confusión Leave-One-Out:\")\n",
    "print(confusion_matrix(y, y_pred_loo))\n",
    "\n",
    "# --- Hold-Out Validation (70-30 split) ---\n",
    "# Dividir en entrenamiento y prueba con un 70-30\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Validar en el conjunto de prueba\n",
    "correct_predictions_holdout = 0\n",
    "y_pred_holdout = []\n",
    "\n",
    "for i, test_point in enumerate(X_test):\n",
    "    predicted_class = classify_1nn(test_point, X_train, y_train)\n",
    "    y_pred_holdout.append(predicted_class)\n",
    "    if predicted_class == y_test[i]:\n",
    "        correct_predictions_holdout += 1\n",
    "\n",
    "# Calcular y mostrar la precisión de Hold-Out\n",
    "accuracy_holdout = correct_predictions_holdout / len(X_test)\n",
    "print(f\"Precisión de Hold-Out Validation (70-30): {accuracy_holdout:.2f}\")\n",
    "\n",
    "# Imprimir la matriz de confusión para Hold-Out\n",
    "print(\"Matriz de Confusión Hold-Out:\")\n",
    "print(confusion_matrix(y_test, y_pred_holdout))\n",
    "\n",
    "# --- 10-Fold Cross-Validation Estratificado ---\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "correct_predictions_kfold = 0\n",
    "total_test_samples = 0\n",
    "y_pred_kfold = []\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    # Dividir datos en entrenamiento y prueba para el fold actual\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    # Validar en el conjunto de prueba\n",
    "    for i, test_point in enumerate(X_test):\n",
    "        predicted_class = classify_1nn(test_point, X_train, y_train)\n",
    "        y_pred_kfold.append(predicted_class)\n",
    "        if predicted_class == y_test[i]:\n",
    "            correct_predictions_kfold += 1\n",
    "    \n",
    "    # Actualizar el conteo de muestras de prueba\n",
    "    total_test_samples += len(X_test)\n",
    "\n",
    "# Calcular y mostrar la precisión de 10-Fold Cross-Validation\n",
    "accuracy_kfold = correct_predictions_kfold / total_test_samples\n",
    "print(f\"Precisión de 10-Fold Cross-Validation Estratificado: {accuracy_kfold:.2f}\")\n",
    "\n",
    "# Imprimir la matriz de confusión para 10-Fold Cross-Validation\n",
    "print(\"Matriz de Confusión 10-Fold Cross-Validation:\")\n",
    "print(confusion_matrix(y, y_pred_kfold))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "K-means-clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
